{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Tuple\n",
    "\n",
    "from env_mp_1 import MetaGames\n",
    "from rmax_1 import RmaxAgent, Memory\n",
    "\n",
    "def discretize(number, radius):\n",
    "    #[0,3,5,4,8] --> [0,3,6,3,9] for radius 3\n",
    "    return torch.round(torch.div(number, radius)) * radius\n",
    "\n",
    "def Boltzmann(tensor):\n",
    "    #0.5 is just a temperature parameter, controls the spread of the softmax distribution\n",
    "    prob = torch.softmax(env.innerq[0,:].cpu()/0.4, 0).numpy()\n",
    "    action_value = np.random.choice(np.arange(tensor.size()[0]), p=prob)\n",
    "    return torch.Tensor([action_value]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_gamma = 0         #inner game discount factor, 0 since it's a one shot game\n",
    "meta_gamma = 0.99          #meta game discount factor\n",
    "meta_alpha = 0.4          #meta game learning rate\n",
    "R_max = 1\n",
    "rmax_error = 0.1\n",
    "meta_epi = 500\n",
    "meta_steps = 500\n",
    "\n",
    "epsilon = 0\n",
    "radius = 0.5                #radius for discretization, assuming radius>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward tensor for plotting purposes [episode, step, agents]\n",
    "plot_rew = torch.zeros(meta_epi, meta_steps, 2).to(device)    \n",
    "\n",
    "# creating environment\n",
    "env = MetaGames()\n",
    "\n",
    "# creating rmax agent\n",
    "memory = Memory()\n",
    "rmax = RmaxAgent(env, R_max, meta_gamma, inner_gamma, radius, epsilon, rmax_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de80323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise meta-state and meta-action randomly\n",
    "meta_s = rmax.index_to_table(env, random.randint(0, rmax.meta_size ** env.num_agents), env.num_agents)\n",
    "memory.states.append(meta_s)\n",
    "for episode in tqdm(range(meta_epi)): #for each meta-episode\n",
    "    #reset environment \n",
    "    env.reset() \n",
    "    print(rmax.nSA)\n",
    "    for step in range(meta_steps):    #for each meta time step\n",
    "        #previous meta-state set as the policy of the next game\n",
    "        env.innerq[0,:] = meta_s[0].detach().clone() \n",
    "        #--------------------------------------START OF INNER GAME--------------------------------------  \n",
    "        #select our inner-action with Boltzmann sampling, oppo inner-action with epsilon greedy \n",
    "        our_action = Boltzmann(env.innerq[0,:].detach().clone())\n",
    "        oppo_action = env.select_action().detach().clone()      \n",
    "        \n",
    "        #print(\"inner actions: \", our_action, oppo_action)\n",
    "        #run inner game according to actions\n",
    "        reward, info = env.step(torch.cat((our_action, oppo_action))) \n",
    "\n",
    "        #update inner r matrix [agent, action]\n",
    "        env.innerr[0, int(our_action)] = reward.detach().clone() \n",
    "        env.innerr[1, int(oppo_action)] = info.detach().clone()\n",
    "        #---------------------------------------END OF INNER GAME--------------------------------------\n",
    "        #save reward, info for plotting              \n",
    "        plot_rew[episode,step,0] = reward.detach().clone()\n",
    "        plot_rew[episode,step,1] = info.detach().clone()\n",
    "        \n",
    "        #Compute new inner Q table, our agent: meta_a that corresponds to max Q(meta_s)/ random, oppo: by Q learning\n",
    "        env.innerq[0, :] = rmax.select_action(env, meta_s[0], -1)\n",
    "        env.innerq[1, :] = (1-meta_alpha) * env.innerq[1, :] + torch.Tensor([meta_alpha * info * oppo_action, meta_alpha * info * (1-oppo_action)]).to(device)\n",
    "        #print(\"inner-r: \", reward, \"\\n inner-q: \", env.innerq)\n",
    "\n",
    "        #meta-action = action that corresponds to max Q(meta_s) = our inner Q\n",
    "        meta_a = env.innerq[0, :]\n",
    "        memory.actions.append(meta_a) \n",
    "\n",
    "        #meta-state = discretized inner game Q table of all agents\n",
    "        new_meta_s = discretize(env.innerq.detach().clone(), radius)\n",
    "        memory.states.append(new_meta_s)    \n",
    "\n",
    "        #meta-reward = sum of rewards of our agent in inner game of K episodes & T timesteps\n",
    "        our_REW = reward.detach().clone()                \n",
    "        memory.rewards.append(our_REW)\n",
    "\n",
    "        #rmax update step\n",
    "        rmax.update(env, memory, meta_s, meta_a, new_meta_s)\n",
    "\n",
    "        #print(\"updating s-a pair:\", rmax.find_meta_index( torch.flatten(meta_s)), rmax.find_meta_index( torch.flatten(meta_a)),\"\\nrmax.R: \", rmax.R, \"\\nrmax.Q: \", rmax.Q, \"\\nrmax.nSA: \", rmax.nSA)\n",
    "        #print(meta_s, meta_a, new_meta_s)\n",
    "\n",
    "        #prepare meta_s for next step\n",
    "        meta_s = new_meta_s.detach().clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see which columns are empty\n",
    "coll = []\n",
    "for i in range(81):\n",
    "    if all(rmax.nSA[i] == torch.zeros(9).to(device)):\n",
    "        coll.append(i) \n",
    "print(len(coll), coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f5e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file and use dump()\n",
    "with open('plot_rew' + str(datetime.now()) + '.pkl', 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(plot_rew, file)\n",
    "    \n",
    "# Open a file and use dump()\n",
    "with open('memory' + str(datetime.now()) + '.pkl', 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(memory, file)\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open('rmax' + str(datetime.now()) + '.pkl', 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(rmax, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a337d9",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f78211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate histogram\n",
    "visit_dict = {}\n",
    "for i in range(len(rmax.nSA.flatten().tolist())):\n",
    "    visit_dict[i]= rmax.nSA.flatten().tolist()[i]\n",
    "    \n",
    "histogram_dict = Counter(visit_dict.values())\n",
    "plt.bar(histogram_dict.keys(), histogram_dict.values(), 0.5, color='g')\n",
    "plt.xlabel(\"visitation counts: \" + str(histogram_dict), fontsize=12)\n",
    "figure0 = plt.gcf()\n",
    "figure0.set_size_inches(10, 8)\n",
    "plt.savefig('histogram at' + str(datetime.now()) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facf934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate reward mean\n",
    "plot_rew_mean = torch.mean(plot_rew[:,:,0],1)\n",
    "fig_handle = plt.plot(plot_rew_mean.cpu().numpy())\n",
    "\n",
    "plt.xlabel(\"episodes \\n Average reward of our agent: \" + str(round(torch.mean(plot_rew[:,:,0],(0,1)).detach().item(), 3)) + \n",
    "          \"\\n Average reward of another agent: \" + str(round(torch.mean(plot_rew[:,:,1],(0,1)).detach().item(), 3)))\n",
    "\n",
    "plt.ylabel(\"Mean rewards\")\n",
    "\n",
    "figure2 = plt.gcf() # get current figure\n",
    "figure2.set_size_inches(10, 8)\n",
    "\n",
    "plt.savefig('inner_gamma' + str(inner_gamma) + '_rad' + str(radius) + '_' + str(meta_epi) + '_' + str(meta_steps) + '_mp1.png'  , dpi = 100)\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate learning curve at start\n",
    "plot_rew_epi_start = torch.mean(plot_rew[:int(meta_epi*0.1), :, 0], 0)\n",
    "fig_handle = plt.plot(plot_rew_epi_start.cpu().numpy())\n",
    "\n",
    "plt.xlabel(\"steps\")\n",
    "\n",
    "plt.ylabel(\"Average learning rate of first \" + str(int(meta_epi*0.1)) + \" episodes\")\n",
    "\n",
    "figure3 = plt.gcf() # get current figure\n",
    "figure3.set_size_inches(10, 8)\n",
    "\n",
    "plt.savefig('inner_gamma' + str(inner_gamma) + '_rad' + str(radius) + '_' + str(meta_epi) + '_' + str(meta_steps) + '_first_epi_mp1.png' , dpi = 100)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd430b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate learning curve at end\n",
    "plot_rew_epi_end = torch.mean(plot_rew[-int(meta_epi*0.1):, :, 0], 0)\n",
    "fig_handle = plt.plot(plot_rew_epi_end.cpu().numpy())\n",
    "\n",
    "plt.xlabel(\"steps\")\n",
    "\n",
    "plt.ylabel(\"Average learning rate of last \" + str(int(meta_epi*0.1)) + \" episodes\")\n",
    "\n",
    "figure4 = plt.gcf() # get current figure\n",
    "figure4.set_size_inches(10, 8)\n",
    "\n",
    "plt.savefig('inner_gamma' + str(inner_gamma) + '_rad' + str(radius) + '_' + str(meta_epi) + '_' + str(meta_steps) + '_last_epi_mp1.png' , dpi = 100)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15965877",
   "metadata": {},
   "source": [
    "# Interpreting results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e32364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path1 = \"memory*.pkl\"\n",
    "path2 = \"plot_rew*.pkl\"\n",
    "path3 = \"rmax*.pkl\"\n",
    "for filename in glob.glob(path1):\n",
    "    with open(filename, 'rb') as f:\n",
    "        memory = pickle.load(f)\n",
    "        \n",
    "for filename in glob.glob(path2):    \n",
    "    with open(filename, 'rb') as g:\n",
    "        plot_rew = pickle.load(g)\n",
    "        \n",
    "for filename in glob.glob(path3):    \n",
    "    with open(filename, 'rb') as g:\n",
    "        rmax = pickle.load(g)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce2b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kittymfos",
   "language": "python",
   "name": "kittymfos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
