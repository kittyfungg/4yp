{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 4 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
      "  (commit or discard the untracked or modified content in submodules)\n",
      "\n",
      "\t\u001b[31mmodified:   ../MFOS\u001b[m (modified content, untracked content)\n",
      "\t\u001b[31mdeleted:    Dict/slurm-2045189.out\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main b23437f] big updatesss: changed fuhnctions so it nolonger chooses the first index that gives the max value\n",
      " 1 file changed, 788 deletions(-)\n",
      " delete mode 100644 Work/Dict/slurm-2045189.out\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"big updatesss: changed functions so it nolonger chooses the first index that gives the max value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Tuple\n",
    "\n",
    "from env_mp_2 import MetaGames\n",
    "from rmax_2 import RmaxAgent, Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_epochs = 4  # update policy for K epochs\n",
    "inner_gamma = 0.8  #inner game discount factor\n",
    "meta_gamma = 0.8   #meta game discount factor\n",
    "R_max = 0.98\n",
    "max_meta_epi = 500\n",
    "max_meta_steps = 50\n",
    "max_inner_epi = 10\n",
    "max_inner_steps = 5\n",
    "\n",
    "epsilon=0.2\n",
    "alpha = 0.4\n",
    "radius_dp =1\n",
    "\n",
    "# creating environment\n",
    "env = MetaGames(\"NL\", \"IPD\")\n",
    "\n",
    "# creating rmax agent\n",
    "memory = Memory()\n",
    "rmax = RmaxAgent(env, R_max, meta_gamma, max_inner_epi, max_inner_steps, radius_dp, epsilon)\n",
    "\n",
    "#action_dim = env.d\n",
    "nA = env.d\n",
    "#state_dim = env.d * 2\n",
    "nS = env.d * 2\n",
    "\n",
    "#all agents' trajectory, entries will be like {[s1, a1, r1], [s2, a2, r2],...}\n",
    "#torch size: {no. of inner episodes, no of inner steps, 3 (sar)}\n",
    "traj = torch.empty(max_inner_epi, max_inner_steps, 3).to(device) \n",
    "our_rew = 0     #our agent's reward\n",
    "oppo_rew = 0   #opponent's reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type list doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m memory\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(meta_r)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#next-meta-state = trajectorty\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m next_meta_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius_dp\u001b[49m\u001b[43m)\u001b[49m          \n\u001b[1;32m     50\u001b[0m memory\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(next_meta_s)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#Rmax update step\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: type list doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "#initialise meta-state = empty trajectory\n",
    "meta_s = torch.empty(max_inner_epi, max_inner_steps, (3+(env.num_agents-1)*2)).to(device)\n",
    "memory.states.append(meta_s)\n",
    "\n",
    "for episode in tqdm(range(max_meta_epi)): #for each meta-episode\n",
    "    #reset environment\n",
    "    env.reset() \n",
    "    \n",
    "    for step in range(max_meta_steps):    #for each meta time step\n",
    "        \n",
    "        #--------------------------------------START OF INNER GAME--------------------------------------            \n",
    "        #for each inner episodes            \n",
    "        for epi in range(max_inner_epi):                 \n",
    "            for t in range(max_inner_steps):                     #for each inner timestep\n",
    "                if t == 0:\n",
    "                    #initialise action by random, of size [agents, batch_size]\n",
    "                    best_action = env.init_action.detach().clone()  \n",
    "                else:\n",
    "                    #find action that has max Q value for current state for both agents\n",
    "                    best_action = env.select_action().detach().clone()     \n",
    "                \n",
    "                #run inner game according to that action, for K episodes & T timesteps, output used to be new_state, reward, done, _ \n",
    "                reward, info = env.step(best_action)  \n",
    "                \n",
    "                #trajectory only comprises of [action, reward]\n",
    "                #traj[epi].append(best_action, [reward,info])   \n",
    "                traj[epi,t,2:] = best_action, reward, info\n",
    "                our_rew += reward.reshape(-1)\n",
    "                oppo_rew += info.reshape(-1)\n",
    "\n",
    "                #update inner r matrix\n",
    "                env.innerr[0, best_action[0]] = reward\n",
    "                env.innerr[1, best_action[1]] = info\n",
    "\n",
    "                env.innerq[0, best_action[0]] = env.innerr[0, best_action[0]] + inner_gamma * torch.max(env.innerq[0, best_action[0]]) \n",
    "                env.innerq[1, best_action[1]] = env.innerr[1, best_action[1]] + inner_gamma * torch.max(env.innerq[1, best_action[1]])\n",
    "                \n",
    "                inner_state = best_action\n",
    "        \n",
    "        #---------------------------------------END OF INNER GAME---------------------------------          \n",
    "        \n",
    "        #meta-action = inner game Q table for our agent\n",
    "        meta_a = env.innerq[0,:]\n",
    "        memory.actions.append(meta_a)\n",
    "        \n",
    "        #meta-reward = sum of inner rewards of our agent over K episodes & T timesteps\n",
    "        meta_r = torch.round(our_rew, decimals = radius_dp)             \n",
    "        memory.rewards.append(meta_r)\n",
    "        \n",
    "        #next-meta-state = trajectorty\n",
    "        next_meta_s = torch.round(traj, decimals = radius_dp).long().to(device)               \n",
    "        memory.states.append(next_meta_s)\n",
    "        \n",
    "        #Rmax update step\n",
    "        rmax.update(env, Memory, meta_s, meta_a, next_meta_s)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(env.innerq, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.init_action.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #find discretized state index that corresponds to the real state value\n",
    "        dstate_index=[]\n",
    "        for j in torch.round(state, decimals = radius_dp):\n",
    "            for i,x in enumerate(ref_arr):\n",
    "                if x==j:\n",
    "                    dstate_index.append(i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN2ZadiniE7gIAA2rNyJFVU",
   "collapsed_sections": [],
   "name": "Rmax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kittymfos",
   "language": "python",
   "name": "kittymfos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
