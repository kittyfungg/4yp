{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb72619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Tuple\n",
    "\n",
    "from env_ipd import MetaGames\n",
    "from rmax import RmaxAgent, Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73f79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_epochs = 4  # update policy for K epochs\n",
    "inner_gamma = 0.9  #inner game discount factor\n",
    "meta_gamma = 0.99   #meta game discount factor\n",
    "R_max = 0.98\n",
    "max_meta_epi = 500\n",
    "max_meta_steps = 5\n",
    "max_inner_epi = 10\n",
    "max_inner_steps = 5\n",
    "max_visits_per_state = 5\n",
    "epsilon=0.2\n",
    "alpha = 0.4\n",
    "bs = 4          #batch size (must need for mfos for good results)\n",
    "radius_dp = 1   #no. of decimal point for discretization \n",
    "\n",
    "our_rew = torch.zeros(bs).to(device)     #our agent's reward\n",
    "oppo_rew = torch.zeros(bs).to(device)    #opponent's reward\n",
    "\n",
    "# creating environment\n",
    "env = MetaGames(bs, \"NL\", \"IPD\")\n",
    "\n",
    "#action_dim = env.num_actions\n",
    "nA = env.num_actions\n",
    "#state_dim = env.num_actions ^ env.d\n",
    "nS = env.num_actions ** env.d\n",
    "#no of possible combinations for a inner Q value, #inner_gamma determins the max value of innerq, radius_dp determines number of intervals\n",
    "poss_combo = math.ceil((10**radius_dp) * (1/(1-gamma)) + 1)\n",
    "\n",
    "\n",
    "memory = Memory()\n",
    "rmax = RmaxAgent(env, R_max, meta_gamma, max_meta_epi, max_meta_steps, radius_dp, epsilon, poss_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80a66918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10828567056280801"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "101**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0310b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta-state = inner game Q table for all agents\n",
    "meta_s = torch.round(env.innerq, decimals = radius_dp).to(device)\n",
    "#meta-action = inner game Q table for our agent\n",
    "meta_a = torch.round(env.innerq[:,:,:,0], decimals = radius_dp).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd79c8ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.3000,  3.2000, 13.5000,  0.0000, 13.3000,  3.3000, 13.5000,  0.0000,\n",
       "         0.0000,  0.0000, 13.2000,  3.4000,  0.0000,  3.2000, 13.5000,  1.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(meta_s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fdc3a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(poss_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec403f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_meta_index(meta, radius_dp, inner_gamma):\n",
    "    #inner_gamma determins the max value of innerq, radius_dp determines number of intervals\n",
    "    poss_combo = math.ceil((10 ** radius_dp) * (1/(1-inner_gamma)) +1)\n",
    "    for i in range(list(meta.size())[0]):\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5154cab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 0\n",
      "timestep = 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got 16, 16x2,4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rmax\u001b[38;5;241m.\u001b[39mmax_steps):    \u001b[38;5;66;03m#for each meta time step\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep =\u001b[39m\u001b[38;5;124m\"\u001b[39m, step)\n\u001b[0;32m----> 6\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m#reset environment  ok\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#for each inner episodes            \u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):                 \n",
      "File \u001b[0;32m~/Desktop/Work/env_ipd.py:49\u001b[0m, in \u001b[0;36mMetaGames.reset\u001b[0;34m(self, info)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     47\u001b[0m     rand_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md))\u001b[38;5;241m.\u001b[39mto(device)        \u001b[38;5;66;03m#random action of size [size.b, size.d], action value either 0 (Coorperate) or 1(Defect)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     state, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrand_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "File \u001b[0;32m~/Desktop/Work/env_ipd.py:54\u001b[0m, in \u001b[0;36mMetaGames.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 54\u001b[0m     l1, l2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     state \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdetach(), \u001b[38;5;241m-\u001b[39ml2\u001b[38;5;241m.\u001b[39mdetach(), \u001b[38;5;241m-\u001b[39ml1\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/Desktop/Work/env_ipd.py:17\u001b[0m, in \u001b[0;36mpd_one_iteration_batched.<locals>.Ls\u001b[0;34m(action)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLs\u001b[39m(action):\n\u001b[1;32m     16\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([action[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39maction[\u001b[38;5;241m0\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mcat([action[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39maction[\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     L_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayout_mat_1\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     18\u001b[0m     L_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(L_1, (bs,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)) \n\u001b[1;32m     19\u001b[0m     L_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmatmul(torch\u001b[38;5;241m.\u001b[39mmatmul(x, payout_mat_2), y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, got 16, 16x2,4"
     ]
    }
   ],
   "source": [
    "for episode in range(rmax.max_episodes): #for each meta-episode\n",
    "    print(\"episode =\", episode)\n",
    "    \n",
    "    for step in range(rmax.max_steps):    #for each meta time step\n",
    "        print(\"timestep =\", step)\n",
    "        state = env.reset()   #reset environment \n",
    "                   \n",
    "        for epi in range(10):              #for each inner episodes   \n",
    "            state = env.reset()   #reset environment \n",
    "            for t in range(50):                     #for each inner timestep\n",
    "                if t == 0:\n",
    "                    #initialise action by random, of size [agents, batch_size]\n",
    "                    best_action = torch.randint(2, (env.d, env.b)).to(device)   \n",
    "                else:\n",
    "                    #find action that has max Q value for current state for both agents\n",
    "                    best_action = env.choose_action(state)   \n",
    "\n",
    "                #run inner game according to that action, for K episodes & T timesteps, output used to be new_state, reward, done, _ \n",
    "                newstate, reward, info = env.step(best_action)  \n",
    "\n",
    "                #update inner r matrix\n",
    "                for i in range(env.b):\n",
    "                    env.innerr[i, state[i], best_action[0,i], 0] = reward[i] \n",
    "                    env.innerr[i, state[i], best_action[1,i], 1] = info[i]\n",
    "\n",
    "                #update agents' total reward\n",
    "                our_rew += reward.reshape(-1)   \n",
    "                oppo_rew += info.reshape(-1)    \n",
    "\n",
    "                #update inner q matrix, another for loop since have to wait till inner r matrix gets updated    \n",
    "                for i in range(env.b):    \n",
    "                    #env.innerq[i, state[i], best_action[0,i], 0] += alpha * (env.innerr[i, state[i], best_action[0,i], 0] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 0]) - env.innerq[i, state[i], best_action[0,i], 0])    \n",
    "                    #env.innerq[i, state[i], best_action[1,i], 1] += alpha * (env.innerr[i, state[i], best_action[1,i], 1] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 1]) - env.innerq[i, state[i], best_action[1,i], 1])\n",
    "                    env.innerq[i, state[i], best_action[0,i], 0] = env.innerr[i, state[i], best_action[0,i], 0] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 0]) \n",
    "                    env.innerq[i, state[i], best_action[1,i], 1] = env.innerr[i, state[i], best_action[1,i], 1] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 1])\n",
    "\n",
    "                #set current state = new state\n",
    "                state = newstate  \n",
    "        \n",
    "        print(\"done 1 inner episode,\", step)\n",
    "        #meta-state = inner game Q table for all agents\n",
    "        meta_s = torch.round(env.innerq, decimals = radius_dp).to(device)\n",
    "        #meta-action = inner game Q table for our agent\n",
    "        meta_a = torch.round(env.innerq[:,:,:,0], decimals = radius_dp).to(device)       \n",
    "        #select meta-action that corresponds to our agent's max Q table\n",
    "        #best_meta_a = torch.argmax(rmax.Q[:,torch.flatten(meta_s)]).to(device)  \n",
    "        our_REW = our_rew                           #meta-reward = sum of rewards of our agent in inner game of K episodes & T timesteps\n",
    "        \n",
    "        for i in range(env.b):\n",
    "            rmax.update(memory, find_meta_index(torch.flatten(meta_a[i])) , find_meta_index(torch.flatten(meta_s[i])))\n",
    "        \n",
    "#             if done:\n",
    "#                 if not(reward==1):\n",
    "#                     self.R[state][best_action]=-10\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b61d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find discretized state index that corresponds to the real state value\n",
    "                #dstate=[]\n",
    "                #for j in torch.round(state, decimals = radius_dp):\n",
    "                #    for i,x in enumerate(ref_arr):\n",
    "                #        if x==j:\n",
    "                #            dstate_index.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kittymfos",
   "language": "python",
   "name": "kittymfos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
