{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Tuple\n",
    "\n",
    "from env_ipd import MetaGames\n",
    "from rmax_2 import RmaxAgent, Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_epochs = 4  # update policy for K epochs\n",
    "inner_gamma = 0.95  #inner game discount factor\n",
    "meta_gamma = 0.99   #meta game discount factor\n",
    "R_max = 0.98\n",
    "max_meta_epi= 500\n",
    "max_meta_steps= 5 \n",
    "max_inner_epi = 3\n",
    "max_inner_steps = 2\n",
    "epsilon=0.2\n",
    "alpha = 0.4\n",
    "bs = 1          #batch size (must need for mfos for good results)\n",
    "radius=1\n",
    "\n",
    "#all agents' trajectory, entries will be like {[s1, a1, r1], [s2, a2, r2],...}\n",
    "#torch size: {no. of inner episodes, no of inner steps, batch size, 3+no. of other agents*2 (2 since they take up 1 in state, 1 in rewards)}\n",
    "traj=torch.empty(max_inner_epi, max_inner_steps, bs, (3+1*2)).to(device) \n",
    "our_rew = torch.zeros(bs).to(device)     #our agent's reward\n",
    "oppo_rew = torch.zeros(bs).to(device)    #opponent's reward\n",
    "\n",
    "# creating environment\n",
    "env = MetaGames(bs, \"NL\", \"IPD\")\n",
    "\n",
    "#action_dim = env.d\n",
    "nA = env.d\n",
    "#state_dim = env.d * 2\n",
    "nS = env.d * 2\n",
    "\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = meta_gamma\n",
    "epsilon = epsilon\n",
    "max_inner_epi = max_inner_epi\n",
    "max_inner_steps = max_inner_steps\n",
    "b = env.b\n",
    "\n",
    "#no of possible combinations for meta-s \n",
    "poss_combo_s = (env.d -1) * env.num_agents * (env.num_actions * 4)    #4 for number of rewards\n",
    "meta_S_size = poss_combo_s ** (max_inner_epi * max_inner_steps * b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones(): argument 'size' must be tuple of SymInts, but found element of type int at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rmax \u001b[38;5;241m=\u001b[39m \u001b[43mRmaxAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_gamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_meta_epi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_meta_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Work/rmax_2.py:43\u001b[0m, in \u001b[0;36mRmaxAgent.__init__\u001b[0;34m(self, env, R_max, gamma, max_inner_epi, max_inner_steps, radius, epsilon)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_A_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39md \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_actions     \u001b[38;5;66;03m#5^2\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#Q for meta-game, *2 for 2 player  \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_S_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_A_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmul(R_max \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma))\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_S_size , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_A_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSA \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_S_size , \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_A_size)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: ones(): argument 'size' must be tuple of SymInts, but found element of type int at pos 2"
     ]
    }
   ],
   "source": [
    "rmax = RmaxAgent(env, R_max, meta_gamma, max_meta_epi, max_meta_steps, radius, epsilon = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones(): argument 'size' must be tuple of SymInts, but found element of type int at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m nS \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39md \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     28\u001b[0m memory \u001b[38;5;241m=\u001b[39m Memory()\n\u001b[0;32m---> 29\u001b[0m rmax \u001b[38;5;241m=\u001b[39m \u001b[43mRmaxAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_gamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_meta_epi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_meta_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Work/rmax_2.py:43\u001b[0m, in \u001b[0;36mRmaxAgent.__init__\u001b[0;34m(self, env, R_max, gamma, max_inner_epi, max_inner_steps, radius, epsilon)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_A_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39md \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_actions     \u001b[38;5;66;03m#5^2\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#Q for meta-game, *2 for 2 player  \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_S_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_A_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmul(R_max \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma))\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSA \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_size)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: ones(): argument 'size' must be tuple of SymInts, but found element of type int at pos 2"
     ]
    }
   ],
   "source": [
    "K_epochs = 4  # update policy for K epochs\n",
    "inner_gamma = 0.95  #inner game discount factor\n",
    "meta_gamma = 0.99   #meta game discount factor\n",
    "R_max = 0.98\n",
    "max_meta_epi= 500\n",
    "max_meta_steps= 5 \n",
    "max_inner_epi = 10\n",
    "max_inner_steps = 5\n",
    "epsilon=0.2\n",
    "alpha = 0.4\n",
    "bs = 2          #batch size (must need for mfos for good results)\n",
    "radius=1\n",
    "\n",
    "#all agents' trajectory, entries will be like {[s1, a1, r1], [s2, a2, r2],...}\n",
    "#torch size: {no. of inner episodes, no of inner steps, batch size, 3+no. of other agents*2 (2 since they take up 1 in state, 1 in rewards)}\n",
    "traj=torch.empty(max_inner_epi, max_inner_steps, bs, (3+1*2)).to(device) \n",
    "our_rew = torch.zeros(bs).to(device)     #our agent's reward\n",
    "oppo_rew = torch.zeros(bs).to(device)    #opponent's reward\n",
    "\n",
    "# creating environment\n",
    "env = MetaGames(bs, \"NL\", \"IPD\")\n",
    "\n",
    "#action_dim = env.d\n",
    "nA = env.d\n",
    "#state_dim = env.d * 2\n",
    "nS = env.d * 2\n",
    "\n",
    "memory = Memory()\n",
    "rmax = RmaxAgent(env, R_max, meta_gamma, max_meta_epi, max_meta_steps, radius, epsilon = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()   #reset environment\n",
    "dstate_index=[]\n",
    "for j in torch.round(state, decimals = radius_dp):\n",
    "    for i,x in enumerate(ref_arr):\n",
    "        #find dstate index for each agent\n",
    "        for num_agents in range(state.size(dim=1)):\n",
    "            if x==j[num_agents-1]:\n",
    "                dstate_index.append(i)\n",
    "best_action = torch.argmax(innerq[:, dstate_index[0], :, 0], dim=1).unsqueeze(1)   \n",
    "\n",
    "#run inner game according to that action, for K episodes & T timesteps, output used to be new_state, reward, done, _ \n",
    "newstate, reward, info, _ = env.step(best_action)  \n",
    "\n",
    "traj[0,0] = torch.cat((state, best_action, reward, info), dim=1)\n",
    "our_rew += reward.reshape(-1)\n",
    "oppo_rew += info.reshape(-1)\n",
    "\n",
    "#update inner r matrix\n",
    "for i in range(env.b):\n",
    "    #env.innerr[i, state[i], best_action[0,i], 0] += (inner_gamma**t) * reward[i] \n",
    "    #env.innerr[i, state[i], best_action[1,i], 1] += (inner_gamma**t) * info[i]\n",
    "    env.innerr[i, state[i], best_action[0,i], 0] = reward[i] \n",
    "    env.innerr[i, state[i], best_action[1,i], 1] = info[i]\n",
    "\n",
    "#update inner q matrix, another for loop since have to wait till inner r matrix gets updated    \n",
    "for i in range(env.b):    \n",
    "    env.innerq[i, state[i], best_action[0,i], 0] = env.innerr[i, state[i], best_action[0,i], 0] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 0]) \n",
    "    env.innerq[i, state[i], best_action[1,i], 1] = env.innerr[i, state[i], best_action[1,i], 1] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 1])\n",
    "\n",
    "#find index of discretized state value, logic: match ref_arr to state values\n",
    "state = newstate\n",
    "# meta_s = torch.round(traj, decimals = radius_dp)                #meta-state = trajectory of all agents\n",
    "# memory.states.append(meta_s)\n",
    "# meta_a = torch.round(innerq[:, :, :, 0], decimals = radius_dp)  #meta-action = Q-table of inner game of our agent\n",
    "# memory.actions.append(meta_a)\n",
    "# meta_r = torch.round(our_rew, decimals = radius_dp)             #meta-reward = sum of inner rewards of our agent over K episodes & T timesteps\n",
    "# memory.rewards.append(meta_r)\n",
    "\n",
    "# best_meta_a = torch.max(meta_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(rmax.max_meta_epi): #for each meta-episode\n",
    "    print(\"episode =\", episode)\n",
    "    #initialise meta-state and meta-action as zeros\n",
    "    \n",
    "    for step in range(rmax.max_meta_steps):    #for each meta time step\n",
    "        print(\"timestep =\", step)\n",
    "        state = env.reset()   #reset environment\n",
    "                    \n",
    "        #for each inner episodes            \n",
    "        for epi in range(max_inner_epi):                 \n",
    "            for t in range(max_inner_steps):                     #for each inner timestep\n",
    "                if t == 0:\n",
    "                    #initialise action by random, of size [agents, batch_size]\n",
    "                    best_action = env.init_action  \n",
    "                else:\n",
    "                    #find action that has max Q value for current state for both agents\n",
    "                    best_action = env.choose_action(state)   \n",
    "                \n",
    "                #run inner game according to that action, for K episodes & T timesteps, output used to be new_state, reward, done, _ \n",
    "                newstate, reward, info, _ = env.step(best_action.reshape(1))  \n",
    "                \n",
    "                traj[epi].append(state, best_action, [reward,info])\n",
    "                our_rew += reward.reshape(-1)\n",
    "                oppo_rew += info.reshape(-1)\n",
    "\n",
    "                #update inner r matrix\n",
    "                for i in range(env.b):\n",
    "                    #env.innerr[i, state[i], best_action[0,i], 0] += (inner_gamma**t) * reward[i] \n",
    "                    #env.innerr[i, state[i], best_action[1,i], 1] += (inner_gamma**t) * info[i]\n",
    "                    env.innerr[i, state[i], best_action[0,i], 0] = reward[i] \n",
    "                    env.innerr[i, state[i], best_action[1,i], 1] = info[i]\n",
    "\n",
    "                #update inner q matrix, another for loop since have to wait till inner r matrix gets updated    \n",
    "                for i in range(env.b):    \n",
    "                    env.innerq[i, state[i], best_action[0,i], 0] = env.innerr[i, state[i], best_action[0,i], 0] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 0]) \n",
    "                    env.innerq[i, state[i], best_action[1,i], 1] = env.innerr[i, state[i], best_action[1,i], 1] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 1])\n",
    "\n",
    "                #find index of discretized state value, logic: match ref_arr to state values\n",
    "                state = newstate\n",
    "                            \n",
    "                \n",
    "        print(\"done 1 inner episode,\", step)\n",
    "        meta_s = torch.round(traj, decimals = radius_dp).long().to(device)               #meta-state = inner game Q table for all agents\n",
    "        memory.states.append(meta_s)\n",
    "        meta_a = touch.amax(env.innerq[:,:,:,0], dim=2)\n",
    "        #meta_a = torch.round(innerq[:,:,:,0], decimals = radius_dp).long().to(device)       #meta-action = inner game Q table for our agent\n",
    "        memory.actions.append(meta_a)\n",
    "        meta_r = torch.round(our_rew, decimals = radius_dp)             #meta-reward = sum of inner rewards of our agent over K episodes & T timesteps\n",
    "        memory.rewards.append(meta_r)\n",
    "        \n",
    "        best_meta_a = torch.argmax(rmax.Q[:,torch.flatten(meta_s)]).to(device)  #select meta-action that corresponds to our agent's maxi Q table\n",
    "        our_REW = our_rew                           #meta-reward = sum of rewards of our agent in inner game of K episodes & T timesteps\n",
    "        rmax.update(memory, torch.flatten(best_meta_a) , torch.flatten(meta_s))\n",
    "        \n",
    "#             if done:\n",
    "#                 if not(reward==1):\n",
    "#                     self.R[state][best_action]=-10\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #find discretized state index that corresponds to the real state value\n",
    "        dstate_index=[]\n",
    "        for j in torch.round(state, decimals = radius_dp):\n",
    "            for i,x in enumerate(ref_arr):\n",
    "                if x==j:\n",
    "                    dstate_index.append(i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN2ZadiniE7gIAA2rNyJFVU",
   "collapsed_sections": [],
   "name": "Rmax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kittymfos",
   "language": "python",
   "name": "kittymfos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
