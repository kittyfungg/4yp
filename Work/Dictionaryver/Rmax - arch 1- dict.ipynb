{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb72619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Tuple\n",
    "\n",
    "from env import MetaGames\n",
    "from rmax_1 import RmaxAgent, Memory\n",
    "\n",
    "def discretize(number, radius):\n",
    "     #return (torch.round(torch.div(number, radius))) * radius\n",
    "     #change not made: originally: [0,3,6,9], now: [0,1,2,3]\n",
    "     return torch.round(torch.div(number, radius)) * radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73f79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_gamma = 0.9         #inner game discount factor\n",
    "meta_gamma = 0.9          #meta game discount factor\n",
    "meta_alpha = 0.4          #meta game learning rate\n",
    "R_max = 0.98\n",
    "max_meta_epi = 500\n",
    "max_meta_steps = 400\n",
    "\n",
    "epsilon = 0.2\n",
    "radius = 3                #radius for discretization, assuming radius>1\n",
    "\n",
    "#reward tensor for plotting purposes [episode, step, agents]\n",
    "plot_rew = torch.zeros(max_meta_epi, max_meta_steps, 2).to(device)    \n",
    "\n",
    "# creating environment\n",
    "env = MetaGames(\"PD\")\n",
    "\n",
    "memory = Memory()\n",
    "rmax = RmaxAgent(env, R_max, meta_gamma, inner_gamma, radius, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c137f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████▏                                          | 166/500 [14:16<51:59,  9.34s/it]"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(max_meta_epi)): #for each meta-episode\n",
    "    #initialise meta-state and meta-action randomly\n",
    "    meta_s = rmax.index_to_table(env, random.randint(0, rmax.meta_size * env.num_agents), env.num_agents)\n",
    "    memory.states.append(meta_s) \n",
    "    \n",
    "    for step in range(max_meta_steps):    #for each meta time step\n",
    "        #previous meta-state set as the policy of the next game\n",
    "        env.innerq = meta_s.detach().clone() #\n",
    "        #--------------------------------------START OF INNER GAME--------------------------------------\n",
    "        #reset environment \n",
    "        env.reset()   \n",
    "\n",
    "        #select inner-action with epsilon greedy \n",
    "        best_action = env.select_action().detach().clone()      \n",
    "\n",
    "        #run inner game according to best_action\n",
    "        reward, info = env.step(best_action)  \n",
    "\n",
    "        #update inner r matrix [agent, action]\n",
    "        env.innerr[0, int(best_action[0])] = reward.detach().clone() \n",
    "        env.innerr[1, int(best_action[1])] = info.detach().clone()\n",
    "\n",
    "        #---------------------------------------END OF INNER GAME--------------------------------------\n",
    "        #save reward, info for plotting              \n",
    "        plot_rew[episode,step,0] += reward.detach().clone()\n",
    "        plot_rew[episode,step,1] += info.detach().clone()\n",
    "        \n",
    "        #meta-action = inner game Q table of our agent\n",
    "        meta_a = env.innerq[0, :].detach().clone()\n",
    "        memory.actions.append(meta_a) \n",
    "        \n",
    "        #Compute new inner Q table, our agent: by Q learning, oppo: max of Q table by putting in previous meta-s\n",
    "        env.innerq[0, :] = (1-meta_alpha) * env.innerq[0, :] + meta_alpha * torch.Tensor([1-int(best_action[1]), int(best_action[1])]).to(device) * info.detach().clone()\n",
    "        \n",
    "        #find index inside dictionary that corresponds to meta_s entry\n",
    "        #array of indices that corresponds to meta_s\n",
    "        stateind_arr = np.where(rmax.Q[\"state\"] == rmax.find_meta_index(torch.flatten(meta_s)))[0]\n",
    "        if len(stateind_arr) == 0:        #if we haven't visited that meta_s before & no record, \n",
    "            #we put random meta_action as inner Q\n",
    "            env.innerq[1, :] = discretize(rmax.index_to_table(env, random.randint(0, rmax.meta_size), 1), radius)\n",
    "        \n",
    "        else:                             #else if we visited that meta_s before\n",
    "            #inner Q is the action that corresponds to max Q(meta_s)\n",
    "            maxQ_ind = np.argmax([rmax.Q[\"Qval\"][stateind_arr]])\n",
    "            env.innerq[1, :] = rmax.index_to_table(env, rmax.Q[action][stateind_arr[maxQ_ind]], 1)\n",
    "            \n",
    "        #meta-state = discretized inner game Q table of all agents\n",
    "        next_meta_s = discretize(env.innerq.detach().clone(), radius)\n",
    "        memory.states.append(next_meta_s)    \n",
    "        \n",
    "        #meta-reward = sum of rewards of our agent in inner game of K episodes & T timesteps\n",
    "        our_REW = reward.detach().clone()                \n",
    "        memory.rewards.append(reward)\n",
    "        \n",
    "        #rmax update step\n",
    "        rmax.update(env, memory, meta_s, meta_a, next_meta_s)\n",
    "\n",
    "        #prepare meta_s for next step\n",
    "        meta_s = next_meta_s.detach().clone()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c74c39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_rew_mean_diff = torch.mean(plot_rew[:,:,0],1) - torch.mean(plot_rew[:,:,1],1)\n",
    "fig_handle = plt.plot(plot_rew_mean_diff.cpu().numpy())\n",
    "\n",
    "plt.xlabel(\"episodes \\n Average reward of our agent: \" + str(round(torch.mean(plot_rew[:,:,0],(0,1)).detach().item(), 3)) + \n",
    "          \"\\n Average reward of another agent: \" + str(round(torch.mean(plot_rew[:,:,1],(0,1)).detach().item(), 3)))\n",
    "\n",
    "plt.ylabel(\"difference in mean rewards\")\n",
    "\n",
    "figure = plt.gcf() # get current figure\n",
    "figure.set_size_inches(10, 8)\n",
    "\n",
    "plt.savefig('inner_gamma' + str(inner_gamma) + '_rad' + str(radius) + '_' + str(max_meta_epi) + '_' + str(max_meta_steps) + '_pd.png' , dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaef4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kittymfos",
   "language": "python",
   "name": "kittymfos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
