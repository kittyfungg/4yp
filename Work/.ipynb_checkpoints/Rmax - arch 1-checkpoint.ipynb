{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb72619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Tuple\n",
    "\n",
    "from env_ipd import MetaGames\n",
    "from rmax import RmaxAgent, Memory\n",
    "\n",
    "def round_func(number, radius):\n",
    "    return (torch.round(torch.div(number, radius))) * radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73f79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_epochs = 4  # update policy for K epochs\n",
    "inner_gamma = 0.9  #inner game discount factor\n",
    "meta_gamma = 0.9   #meta game discount factor\n",
    "R_max = 0.98\n",
    "max_meta_epi = 500\n",
    "max_meta_steps = 5\n",
    "max_inner_epi = 10\n",
    "max_inner_steps = 5\n",
    "\n",
    "epsilon = 0.2\n",
    "alpha = 0.4\n",
    "bs = 2          #batch size (must need for mfos for good results)\n",
    "radius = 3   #radius for discretization, assuming radius>1\n",
    "\n",
    "plot_rew = torch.zeros(max_inner_epi, max_inner_steps, 2, bs).to(device)    #reward tensor for plotting purposes\n",
    "\n",
    "# creating environment\n",
    "env = MetaGames(bs, \"NL\", \"IPD\")\n",
    "\n",
    "memory = Memory()\n",
    "rmax = RmaxAgent(env, R_max, meta_gamma, max_meta_epi, max_meta_steps, radius, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5154cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-episode = 0\n",
      "meta-timestep = 0\n",
      "done 1 inner episode, 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 481686387163136 is out of bounds for dimension 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m our_REW \u001b[38;5;241m=\u001b[39m reward                           \u001b[38;5;66;03m#meta-reward = sum of rewards of our agent in inner game of K episodes & T timesteps\u001b[39;00m\n\u001b[1;32m     55\u001b[0m memory\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mrmax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_meta_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m meta_s \u001b[38;5;241m=\u001b[39m new_meta_s\n\u001b[1;32m     60\u001b[0m meta_a \u001b[38;5;241m=\u001b[39m new_meta_a\n",
      "File \u001b[0;32m~/Desktop/Work/rmax.py:65\u001b[0m, in \u001b[0;36mRmaxAgent.update\u001b[0;34m(self, memory, state, action, next_state)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSA[i][state_mapped][action_mapped] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[i][state_mapped][action_mapped] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][i]\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSAS[i][state_mapped][action_mapped][next_state_mapped] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSA[i][state_mapped][action_mapped] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mnumber):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 481686387163136 is out of bounds for dimension 0 with size 10"
     ]
    }
   ],
   "source": [
    "for episode in range(rmax.max_episodes): #for each meta-episode\n",
    "    print(\"meta-episode =\", episode)\n",
    "    #initialise meta-state and meta-action as zeros\n",
    "    meta_s = torch.zeros(env.b, env.d, env.num_actions, env.num_agents).to(device) \n",
    "    memory.states.append(meta_s)\n",
    "    meta_a = torch.zeros(env.b, env.d, env.num_actions).to(device) \n",
    "    memory.actions.append(meta_a)    \n",
    "    \n",
    "    for step in range(rmax.max_steps):    #for each meta time step\n",
    "        print(\"meta-timestep =\", step)\n",
    "        \n",
    "        for epi in range(max_inner_epi):              #for each inner episodes \n",
    "            #print(\"inner episode =\", epi)\n",
    "            state = env.reset()   #reset environment \n",
    "            \n",
    "            for t in range(max_inner_steps):                     #for each inner timestep\n",
    "                #print(\"inner timestep =\", t)\n",
    "                if t == 0:\n",
    "                    #initialised action \n",
    "                    best_action = env.init_action\n",
    "                else:\n",
    "                    #find action that has max Q value for current state for both agents\n",
    "                    best_action = env.choose_action(state)   \n",
    "\n",
    "                #run inner game according to that action, for K episodes & T timesteps, output used to be new_state, reward, done, _ \n",
    "                newstate, reward, info = env.step(best_action, t)  \n",
    "                plot_rew[epi,t,0,:] = reward\n",
    "                plot_rew[epi,t,1,:] = info\n",
    "                \n",
    "                #update inner r matrix\n",
    "                for i in range(env.b):\n",
    "                    #env.innerr[i, state[i], best_action[0,i], 0] += (inner_gamma**t) * reward[i] \n",
    "                    #env.innerr[i, state[i], best_action[1,i], 1] += (inner_gamma**t) * info[i]\n",
    "                    env.innerr[i, state[i], best_action[0,i], 0] = reward[i] \n",
    "                    env.innerr[i, state[i], best_action[1,i], 1] = info[i]\n",
    "\n",
    "                #update inner q matrix, another for loop since have to wait till inner r matrix gets updated    \n",
    "                for i in range(env.b):    \n",
    "                    env.innerq[i, state[i], best_action[0,i], 0] = env.innerr[i, state[i], best_action[0,i], 0] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 0]) \n",
    "                    env.innerq[i, state[i], best_action[1,i], 1] = env.innerr[i, state[i], best_action[1,i], 1] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 1])\n",
    "\n",
    "                #set current state = new state\n",
    "                state = newstate  \n",
    "                \n",
    "                #print(\"reward at inner timestep = \", t, \":\", plot_rew[epi,t,:,:])\n",
    "                 \n",
    "        print(\"done 1 inner episode,\", step)\n",
    "        #meta-state = inner game Q table for all agents\n",
    "        new_meta_s = round_func(env.innerq, radius)\n",
    "        #meta-action = inner game Q table for our agent\n",
    "        new_meta_a = round_func(env.innerq[:,:,:,0], radius)      \n",
    "        #select meta-action that corresponds to our agent's max Q table\n",
    "         \n",
    "        our_REW = reward                           #meta-reward = sum of rewards of our agent in inner game of K episodes & T timesteps\n",
    "        memory.rewards.append(reward)\n",
    "        \n",
    "        rmax.update(memory, meta_s, meta_a, new_meta_s)\n",
    "        \n",
    "        meta_s = new_meta_s\n",
    "        meta_a = new_meta_a\n",
    "        \n",
    "#             if done:\n",
    "#                 if not(reward==1):\n",
    "#                     self.R[state][best_action]=-10\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d94ee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "step() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     best_action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mchoose_action(state)   \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#run inner game according to that action, for K episodes & T timesteps, output used to be new_state, reward, done, _ \u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m newstate, reward, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     13\u001b[0m plot_rew[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     14\u001b[0m plot_rew[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;241m=\u001b[39m info\n",
      "\u001b[0;31mTypeError\u001b[0m: step() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "state = env.reset()   #reset environment \n",
    "for t in range(max_inner_steps):                     #for each inner timestep\n",
    "    #print(\"inner timestep =\", t)\n",
    "    if t == 0:\n",
    "        #initialised action \n",
    "        best_action = env.init_action\n",
    "    else:\n",
    "        #find action that has max Q value for current state for both agents\n",
    "        best_action = env.choose_action(state)   \n",
    "\n",
    "    #run inner game according to that action, for K episodes & T timesteps, output used to be new_state, reward, done, _ \n",
    "    newstate, reward, info = env.step(best_action)  \n",
    "    plot_rew[0,0,0,:] = reward\n",
    "    plot_rew[0,0,1,:] = info\n",
    "    print(\"best_action\", best_action)\n",
    "    print(\"reward\", reward)\n",
    "    print(\"newstate\", newstate)\n",
    "\n",
    "    #update inner r matrix\n",
    "    for i in range(env.b):\n",
    "        #env.innerr[i, state[i], best_action[0,i], 0] += (inner_gamma**t) * reward[i] \n",
    "        #env.innerr[i, state[i], best_action[1,i], 1] += (inner_gamma**t) * info[i]\n",
    "        env.innerr[i, state[i], best_action[0,i], 0] = reward[i] \n",
    "        env.innerr[i, state[i], best_action[1,i], 1] = info[i]\n",
    "\n",
    "    #update inner q matrix, another for loop since have to wait till inner r matrix gets updated    \n",
    "    for i in range(env.b):    \n",
    "        env.innerq[i, state[i], best_action[0,i], 0] = env.innerr[i, state[i], best_action[0,i], 0] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 0]) \n",
    "        env.innerq[i, state[i], best_action[1,i], 1] = env.innerr[i, state[i], best_action[1,i], 1] + inner_gamma * torch.max(env.innerq[i, newstate[i], :, 1])\n",
    "\n",
    "    #set current state = new state\n",
    "    state = newstate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33074e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7778b931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.8000, 0.8000],\n",
       "          [0.8000, 1.8000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]]]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872f067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kittymfos",
   "language": "python",
   "name": "kittymfos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
