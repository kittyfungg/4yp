{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb72619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from ipd_environment import MetaGames\n",
    "from rmax import RmaxAgent, Memory\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4acdf4d9",
   "metadata": {},
   "source": [
    "############################################\n",
    "K_epochs = 4  # update policy for K epochs\n",
    "\n",
    "gamma = 0.99  # discount factor\n",
    "R_max = \n",
    "max_episodes=500\n",
    "max_steps=5\n",
    "epsilon=0.2\n",
    "alpha = 0.4\n",
    "bs = 8          #batch size (mus need for mfos for good results)\n",
    "radius_dp = 1   #no. of decimal point for discretization \n",
    "\n",
    "our_rew = torch.zeros(bs).to(device)     #our agent's reward\n",
    "oppo_rew = torch.zeros(bs).to(device)    #opponent's reward\n",
    "\n",
    "# creating environment\n",
    "#env = MetaGames(b, opponent=args.opponent, game=args.game, mmapg_id=args.mamaml_id)\n",
    "env = MetaGames(bs, \"NL\", \"IPD\", 0)\n",
    "innerr = torch.zeros(bs, 10**(radius_dp)+1, env.num_actions, 2).to(device)      #reward table with discretized dimensions, (batch_size, discretized states, actions, player)\n",
    "innerq = torch.zeros(bs, 10**(radius_dp)+1, env.num_actions, 2).to(device)\n",
    "ref_arr = np.linspace(0, 1, 10**(radius_dp)+1)\n",
    "\n",
    "#action_dim = env.d\n",
    "nA = env.d\n",
    "#state_dim = env.d * 2\n",
    "nS = env.d * 2\n",
    "\n",
    "memory = Memory()\n",
    "rmax = RmaxAgent(env, R_max, gamma, max_episodes, max_steps, radius_dp, epsilon = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5154cab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m best_ACTION \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(rmax\u001b[38;5;241m.\u001b[39mQ)  \u001b[38;5;66;03m#select meta-action that corresponds to our agent's maxi Q table\u001b[39;00m\n\u001b[1;32m     44\u001b[0m our_REW \u001b[38;5;241m=\u001b[39m our_rew                           \u001b[38;5;66;03m#meta-reward = sum of rewards of our agent in inner game of K episodes & T timesteps\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mrmax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_ACTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTATE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Work/rmax.py:51\u001b[0m, in \u001b[0;36mRmaxAgent.update\u001b[0;34m(self, memory, best_action, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, memory, best_action, state):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnSA\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m[best_action] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_visits_per_state:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSA[state][best_action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[state][best_action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "for episode in range(rmax.max_episodes): #for each meta-episode\n",
    "    \n",
    "    for step in range(rmax.max_steps):    #for each meta time step\n",
    "        state = env.reset()   #reset environment\n",
    "        #find discretized state index that corresponds to the real state value\n",
    "        dstate_index=[]\n",
    "        for j in torch.round(state, decimals = radius_dp):\n",
    "            for i,x in enumerate(ref_arr):\n",
    "                if x==j:\n",
    "                    dstate_index.append(i)\n",
    "                    \n",
    "        #for each inner episodes            \n",
    "        for epi in range(10):                 \n",
    "            for t in range(5):                     #for each inner timestep\n",
    "                #find action that has max Q value for current state of our agent --> thats why [0]\n",
    "                #the index represents which action to take already since actions are discrete??????? ASKKKKK\n",
    "                best_action = torch.argmax(innerq[:, dstate_index[0], :, 0])     \n",
    "                #run inner game according to that action, for K episodes & T timesteps, output used to be new_state, reward, done, _ \n",
    "                newstate, reward, info, _ = env.step(best_action.reshape(1))  \n",
    "                our_rew += reward.reshape(-1)\n",
    "                oppo_rew += info.reshape(-1)\n",
    "                \n",
    "                innerr[:, dstate_index[0], best_action, 0] += our_rew \n",
    "                innerr[:, dstate_index[1], best_action, 1] += oppo_rew\n",
    "\n",
    "                #find index of discretized state value, logic: match ref_arr to state values\n",
    "                state = newstate\n",
    "                #find discretized state index that corresponds to the real state value\n",
    "                dstate=[]\n",
    "                for j in torch.round(state, decimals = radius_dp):\n",
    "                    for i,x in enumerate(ref_arr):\n",
    "                        if x==j:\n",
    "                            dstate_index.append(i)\n",
    "                            \n",
    "                #update Q table for our agent\n",
    "                innerq[:, dstate_index[0], best_action, 0] += alpha * (innerr[:, dstate_index[0], best_action, 0] + rmax.gamma * torch.amax(innerq[:, :, :, 0], dim=(1,2)) - innerq[:, dstate_index[0], best_action, 0])    \n",
    "                #update Q table for opponent\n",
    "                innerq[:, dstate_index[1], best_action, 1] += alpha * (innerr[:, dstate_index[1], best_action, 1] + rmax.gamma * torch.amax(innerq[:, :, :, 1], dim=(1,2)) - innerq[:, dstate_index[1], best_action, 1])        \n",
    "        \n",
    "        STATE = torch.round(innerq, decimals = radius_dp)                 #meta-state = inner game Q table for all agents\n",
    "        ACTION = torch.round(innerq[:,:,:,0], decimals = radius_dp)       #meta-action = inner game Q table for our agent\n",
    "        \n",
    "        best_ACTION = torch.max(rmax.Q)  #select meta-action that corresponds to our agent's maxi Q table\n",
    "        our_REW = our_rew                           #meta-reward = sum of rewards of our agent in inner game of K episodes & T timesteps\n",
    "        #rmax.update(memory, best_ACTION, STATE)\n",
    "        \n",
    "#             if done:\n",
    "#                 if not(reward==1):\n",
    "#                     self.R[state][best_action]=-10\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43ff772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [-57.5000, -60.1000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000]],\n",
       "\n",
       "        [[  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [-57.5000, -60.1000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000]],\n",
       "\n",
       "        [[  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [-57.5000, -60.1000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000]],\n",
       "\n",
       "        [[  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [-57.5000, -60.1000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000]],\n",
       "\n",
       "        [[  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [-57.5000, -60.1000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000]],\n",
       "\n",
       "        [[  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [-57.5000, -60.1000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000]],\n",
       "\n",
       "        [[  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [-57.5000, -60.1000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000]],\n",
       "\n",
       "        [[  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [-57.5000, -60.1000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd4da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kittymfos",
   "language": "python",
   "name": "kittymfos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
